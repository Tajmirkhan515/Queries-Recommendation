# -*- coding: utf-8 -*-
"""CompleteExperiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xpIwhJCG4F6tJUWCdLE7sQFyvH9EnreH
"""
import sys
import time

import pandas as pd

print("mounted called:")
# Commented out IPython magic to ensure Python compatibility.
#generate summary of text inside the whole df
import sumy
import nltk
#nltk.download('punkt')

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lex_rank import LexRankSummarizer
from sumy.summarizers.lsa import LsaSummarizer
import sys
from multiprocessing import Process
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')
from selenium import webdriver
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
from selenium import webdriver
#search url in text
import re
from selenium import webdriver
import time
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
import time
import pandas as pd 
from Prop.DownloadTextVideoImage import *
#%cd question_generation
from Prop.pipelines import pipeline

#nlp = pipeline("question-generation")
nlp = pipeline("e2e-qg")

# for similarity of Query and Queries
from sentence_transformers import SentenceTransformer, util
import torch
embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')


df_images = pd.DataFrame(columns = [ 'title', 'description'])   
df_doc = pd.DataFrame(columns = ['link', 'title', 'short_description' ,'description']) 
df_video = pd.DataFrame(columns = ['link', 'title', 'description'])        


def loadText_Images(terms):
  #Extract Text from Images
  print("text extracting from images.....IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII") 
  query=terms
  driver_img = webdriver.Chrome('chromedriver',options =chrome_options)
  driver_img.implicitly_wait(5)
  search_url = "https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img"
  driver_img.get(search_url.format(q=query))

  time.sleep(2)
  for i in range(1, 10): # replace 4 to 10
      try:
          id=str(i)
          WebDriverWait(driver_img, 4).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="islrg"]/div[1]/div['+id+']/a[1]/div[1]/img'))).click()      
          time.sleep(2)   
          i_title=driver_img.find_element_by_xpath('//*[@id="Sva75c"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[3]/div[1]/a[1]/div[1]/div').text
          i_description=driver_img.find_element_by_xpath('//*[@id="Sva75c"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[3]/div[1]/a[1]/div[2]').text
          print("Image TITLE:  "+driver_img.find_element_by_xpath('//*[@id="Sva75c"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[3]/div[1]/a[1]/div[1]/div').text)      
          df_images.loc[len(df_images)] = [ i_title, i_description]
          #driver_img.back()        
      except Exception as a:
          print("error")
          print(a)
      
  print("Images dataframe ",df_images)
  driver_img.quit()
  return df_images


def generateSummaryUsingTermBased(str_data,numberofsentence):
  #Generate summary method
  parser = PlaintextParser.from_string(str_data,Tokenizer("english"))
  summarizer_lsa = LsaSummarizer()
  summary_2 =summarizer_lsa(parser.document,numberofsentence)  
  summaryGenerated=" "
  for sentence in summary_2:
      summaryGenerated=summaryGenerated+ str(sentence)
  return summaryGenerated



def loadText_Text(terms):

  global df_doc
  print("Text Extracting Text...................TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT")
  success=GoogleSpider().searchOnlyTenText(terms)
  try:
    df_doc['link'] = success["url"]
  except: 
    print("error link loatTextTex")
  try:  
    df_doc['title'] = success["title"]
  except: 
    print("error Title loatTextTex")
  try:
    df_doc['short_description']=success["des"]
  except: 
    print("error description loatTextTex")
  try:
    df_doc['description'] =success["ful_des"]
  except:
    print("description loatTextTex")
  print("complete data frame of text")
  print(df_doc)     
  return df_doc

def loadText_VideoNew(terms):
  success=GoogleSpider().videoDataSecondTime(terms)
  a=1
  for item in success:
    df_video.loc[a]=item
    a+=1
  return df_video

def loadText_ImagesNew(terms):
  success=GoogleSpider().imageDataSecondTime(terms)
  a=1
  for item in success:
    df_images.loc[a]=item
    a+=1
  return df_images



def loadText_Video(terms):
  #print("Video Text extracting.....vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv")
  
  query=terms
  # Extract Text From youtube Video
  driver_vd = webdriver.Chrome('chromedriver',options =chrome_options)
  query.replace(" ", "+")

  driver_vd.get('https://www.google.com/search?q='+query+'&source=lnms&tbm=vid&sa=X&ved=2ahUKEwik97WF8ODyAhVS_rsIHWCaBKMQ_AUoAXoECAEQAw&biw=1034&bih=576')
  time.sleep(8)
  a=0
  vid_title=['What is the world wide web? - Twila Camp - YouTube',
  'A brief history of the World Wide Web - YouTube',
  'The World Wide Web: A global information space - Science',
  'World Wide Web: How the Web Works - Study.com',
  'World Wide Web Timeline - Pew Research Center',
  'World Wide Web developer wants internet remake',
  'Birth of the World Wide Web - YouTube',
  'The World Wide Web: Crash Course Computer Science #30'
  ]
  vid_desc=['But what is it exactly? Twila Camp describes this interconnected information system as a virtual city that',
  'This video summarises in 3 minutes how the Web was invented at CERN by British physicist and IT expert Tim',
  'The web became available for universal use on 30 April 1993, when CERN published a statement making the',
'Wendy: Of course! The World Wide Web, commonly referred to as the Web, is a system of interlinked, hypertext ',
'Since its founding in 1989, the World Wide Web has touched the lives of billions of people around the world',
'Meet Sir Tim Berners-Lee: First he designed the World Wide Web, now he redesigning it',
'CHM Exhibition "Revolution: The First 2000 Years of Computing"Visionaries dreamed of computerizing and',
'Today we\'re going to discuss the World Wide Web - not to be confused with the Internet, which is the'
  ]
  for i in range(0,10): #replace 4 to 11
      try:
        time.sleep(2)
        a+=1
        v_title=driver_vd.find_element_by_xpath('//*[@id="rso"]/div['+str(a)+']/div/div/div[1]/a/h3').text
        v_description=driver_vd.find_element_by_xpath('//*[@id="rso"]/div['+str(a)+']/div/div/div[2]/div[2]/span/span').text
        df_video.loc[a] = ["empty", v_title, v_description]
        #df_video.loc[a]=["empty", vid_title[i], vid_desc[i]]
      except:
        print(" video Error")
        print()
  
  print("Vudeo meethod dataframe: ",df_video)
  driver_vd.quit
  return df_video
  

def generateSummary_Video():
  #Genereate summary of Video, Description Data
  for i in df_video.index:
    print(df_video.at[i,'description'])
    if df_video.at[i,'description'].count(' ')>250: # on average we generate summary which paragraph is greater than 150
      largeText=df_video.at[i,'description']
      summaryGenerated=generateSummaryUsingTermBased(largeText,5)
      print(summaryGenerated)
      df_video.at[i,'description']=summaryGenerated

def generateSummary_Text():  
  #Generate summary for text Documents
  global df_doc
  df_doc=pd.read_csv('/content/drive/MyDrive/Browser/df_doc_filename.csv')
 
  for i in df_doc.index:
    #print(df_video.at[i,'description'])
    if df_doc.at[i,'description'].count(' ')>260: # on average we generate summary which paragraph is greater than 150
      largeText=df_doc.at[i,'description']
      print("Number of words : ",largeText.count(' '))
      summaryGenerated=generateSummaryUsingTermBased(largeText,6)
      print("sumarray******************************")
      #print(df_doc.at[i,'link'])
      print(summaryGenerated)
      print("Number of words : ",summaryGenerated.count(' '))
      df_doc.at[i,'description']=summaryGenerated
      #print("sumarray******************************")
 
  df_doc.to_csv('df_doc_filename.csv', index = False)

def callAllSummaryMethod():
  print("complete summary method call:")
  #generateSummary_Video()
  #generateSummary_Text()

#preprocessing method
def Preprocessingftn(text):
    """Remove URLs from a text string"""
    text=re.sub(r"http\S+", " ", text)
    text = re.sub(r"[-():\"#â†’*@/;:<>{}`+=~|!,]", " ", text)
    text=re.sub(r"\n+", ".", text)
    text=re.sub(r"&nbsp", ".",text)
    return text

sentences_space=[]

def extractImportantSentences_video(df_video):
  #extract keywords from video
  checkNumberOfSenetences=0
  global sentences_space
  sentences_space.clear()
  for i in df_video.index:
    try:
      largeText=df_video.at[i,'description']+ ". " +df_video.at[i,'title']+". "
      print("index :"+str(i))
      largeText= Preprocessingftn(str(largeText))
      print(largeText)
      #largeText=extringKeywords(largeText)
      liss=nlp(largeText)
      print(liss)
      checkNumberOfSenetences+=len(liss)
      sentences_space.extend(liss)
    except Exception as a:
      print(a)
  print("totalVideo: ",checkNumberOfSenetences)   
  print("Video :77777777777: ",len(sentences_space))


def extractImportantSentences_text(df_doc):
  #Extract keywords from documents
  global sentences_space
  checkNumberOfSenetences=0
  for i in df_doc.index:
    largeText=df_doc.at[i,'title']
    largeText+=str(df_doc.at[i,'short_description'])
    largeText+=df_doc.at[i,'description']

    print("index :"+str(i))
    largeText= Preprocessingftn(str(largeText))
    print(largeText)
    liss=nlp(largeText)
    print(df_doc.at[i,'title'],"=== length===", len(liss))
    checkNumberOfSenetences+=len(liss)    
    sentences_space.extend(liss)
  print("totalText: ",checkNumberOfSenetences)   
  print(" Text:77777777777: ",len(sentences_space))


def extractImportantSentences_image(df_images):
  #Extract keywords from images data
  global sentences_space
  print(df_images['title'])
  checkNumberOfSenetences=0
  for i in df_images.index:
    try:
      if i==10:
        break
      largeText=df_images.at[i,'title']+". "+df_images.at[i,'description']+". "
      print("index :"+str(i), largeText)
      largeText= Preprocessingftn(str(largeText))
      print(largeText)
      #largeText=extringKeywords(largeText)
      liss=nlp(largeText)
      print('image text: ',liss)
      print(len(liss))
      checkNumberOfSenetences+=len(liss)
      sentences_space.extend(liss)
    except Exception as e:
      print("Loading took too much time!")
  print("total_Image: ",checkNumberOfSenetences)   
  print("Images:77777777777: ",len(sentences_space))



def callImportantSentenceMethods():
  print("call ImportansSetenceMethod call: $$$$$$$$$$$")
  extractImportantSentences_video()
  extractImportantSentences_text()
  extractImportantSentences_image()


def numberOfImportantSentencesInAll():
  print("77777777777777:Total Number of sentences:")
  print(len(sentences_space))
  for k in sentences_space:
    print(k)





def findSimilarityBetweenQueryAndQueries(query):
  unique_result=[]
  print("\n\n\n find similarity call")
  print("Total Number Of sentences: ",len(sentences_space))
  # Corpus with example sentences
  #corpus = ['A man is eating food.']
  corpus=sentences_space
  corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)
  # Query sentences:
  queries = []
  queries.append(query)
  # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
  top_k = min(len(corpus), len(corpus))
  print("top k : ",top_k)
  print("top k: "+str(top_k))
  for query in queries:
      query_embedding = embedder.encode(query, convert_to_tensor=True)
      # We use cosine-similarity and torch.topk to find the highest 5 scores
      cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]
      top_results = torch.topk(cos_scores, k=top_k)
      
      for score, idx in zip(top_results[0], top_results[1]):
          print(corpus[idx], "(Score: {:.4f})".format(score))
          try:
            sente=str(corpus[idx]).lower()
            if sente in unique_result:
              print(" ")
              continue
            else:
              if (score>=0.50):
                unique_result.append(sente)
          except Exception as e:
            print("Error")
            print("Exception Here in method : ",e)
  
     
  #unique_res = set(unique_result)

  print("\n\n\nfinal quries length is ",len(unique_result))
  for t in unique_result:
    print(t)
  
  return unique_result

# just for cary the text

def runCompletCode(terms):
  retrievAllTextFromAllType(terms)
  callAllSummaryMethod()
  callImportantSentenceMethods()
  listt=findSimilarityBetweenQueryAndQueries(str(terms))
  return listt
